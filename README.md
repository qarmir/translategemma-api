# TranslateGemma FastAPI Service

Production-ready FastAPI service for **Google TranslateGemma** (text â†’ text and image â†’ text/translation)  
with support for:

- ðŸ§  **GPU sharding (1+ GPUs)**
- âš¡ **Quantization**: none / int8 / int4 (bitsandbytes)
- ðŸŒŠ **SSE token streaming**
- ðŸ§© **YAML + .env configuration** (pydantic-settings)
- ðŸ–¼ **Text translation from images via URL**
- ðŸ”’ Production-friendly design (rate limiting, metrics, proxies, restarts)

---

## Features

- `/translate` â€” standard text translation
- `/translate/stream` â€” translation with **SSE token streaming**
- `/translate_image_url` â€” translate text from an image (URL)
- `/translate_image_url/stream` â€” streaming translation from an image
- `/health` â€” health check + GPU/config diagnostics

Supports:
- **Single GPU / Multi-GPU / CPU offload**
- **BF16 / FP16**
- **Gated Hugging Face models (via HF token)**

---

## Requirements

### Minimum
- Python **3.11+**
- Linux x86_64
- Hugging Face account + model access
- NVIDIA GPU (optional, but recommended)

### Recommended
- CUDA 12.x
- RTX 3090 / A100 / L40 / H100
- 64GB+ RAM (if using CPU offload)

---

## Supported Models

Default:
- `google/translategemma-12b-it`
- `google/translategemma-27b-it`

You can use any compatible **image-text-to-text** model from Hugging Face.

---

## Installation

### 1. Install Poetry
```bash
curl -sSL https://install.python-poetry.org | python3 -
export PATH="$HOME/.local/bin:$PATH"
```

### 2. Clone repository
```bash
git clone https://github.com/qarmir/translategemma-api.git
cd translategemma-api
```

### 3. Install Python 3.11 (or higher)
```bash
sudo apt install -y python3.11 python3.11-venv
poetry env use python3.11
```

### 4. Install dependencies
```bash
poetry install
```

---

## Hugging Face Access
TranslateGemma models are gated.
You must accept the license and generate a token on Hugging Face:

https://huggingface.co/google/translategemma-12b-it

---

## Configuration
```bash
# Model
TG_MODEL__ID=google/translategemma-12b-it
TG_MODEL__GPU_COUNT=1         # How many GPUs to use
TG_MODEL__QUANTIZATION=int8   # None for no quantization
TG_MODEL__MAX_MEMORY_PER_GPU=23GiB
TG_MODEL__MAX_MEMORY_CPU=128GiB

# Service
TG_SERVICE__MAX_CONCURRENT=1

# Generation
TG_GENERATION__DEFAULT_MAX_NEW_TOKENS=200
TG_GENERATION__MAX_NEW_TOKENS_LIMIT=1024
```

---

## Running the service
```bash
export HF_TOKEN=hf_token_here
poetry run serve
```

---

## Health Check
```bash
curl http://localhost:8000/health | jq .
```

---

## Usage
### Standard Translation
```bash
curl http://localhost:8000/translate \
  -H "content-type: application/json" \
  -d '{
    "text": "Hello world",
    "source_lang_code": "en",
    "target_lang_code": "de"
  }'
```

---

## SSE Streaming
### Text
```bash
curl -N http://localhost:8000/translate/stream \
  -H "content-type: application/json" \
  -d '{
    "text": "Translate into Russian: I will be late for the meeting.",
    "source_lang_code": "en",
    "target_lang_code": "ru"
  }'
```

### Image URL

```bash
curl -N http://localhost:8000/translate_image_url/stream \
  -H "content-type: application/json" \
  -d '{
    "image_url": "https://example.com/text_image.png",
    "source_lang_code": "en",
    "target_lang_code": "fr"
  }'
```

---

## GPU Modes

| Mode               | VRAM Usage | Recommendation  |
| ------------------ | ---------- | --------------- |
| `none` (bf16/fp16) | ðŸ”´ High    | Multi-GPU only  |
| `int8`             | ðŸŸ¡ Medium  | Best balance    |
| `int4`             | ðŸŸ¢ Low     | Maximum density |

### 12B Model

* 1Ã—3090 â†’ `int8` / `int4`
* 2Ã—3090 â†’ `bf16` or `int8`

### 27B Model

* 2Ã—3090 â†’ `int4` / `int8`
* A100 80GB â†’ `bf16`

---

## License

Model: Google TranslateGemma â€” see license on Hugging Face
Service code: MIT / Apache-2.0 (your choice)

---